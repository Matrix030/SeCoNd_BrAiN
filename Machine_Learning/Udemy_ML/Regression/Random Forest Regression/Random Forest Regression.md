![[Pasted image 20250610104227.png]]
## ðŸŒ² Random Forests for Regression â€“ Intuition & Overview
### ðŸ“˜ What Is a Random Forest?
> A **Random Forest** is an **ensemble learning** method that builds **multiple decision trees** and **aggregates their results** to make better predictions.

In the regression setting:
- Each tree makes a **numerical prediction**.
- The final output is the **average** of all tree predictions.
### ðŸ¤– Ensemble Learning: The Bigger Idea
> **Ensemble learning** = combining multiple models to make better decisions than any single model could alone.

Popular ensemble methods:
- Random Forests
- Gradient Boosting
- Bagging, etc.

## ðŸ› ï¸ How Random Forests Work (Regression Focus)
### Step-by-Step Process:
1. **Sample K data points** (randomly) from the training set  
    â†’ This is called **bootstrapping**
2. **Train one regression tree** on this sample
3. **Repeat** steps 1â€“2 multiple times (typically 100s of times)
4. **Predict** for a new data point by:
    - Sending it through all trees
    - Taking the **average** of all predicted y-values
### ðŸ“ˆ Why Is This Better?
#### âœ… Accuracy
- A single tree might overfit or underperform.
- Averaging predictions across trees **smooths out errors**.

#### âœ… Stability
- One decision tree is sensitive to data changes.
- A forest of trees is **robust to noise and outliers**.
## ðŸŽ¯ Analogy: Guessing Jelly Beans in a Jar
> Imagine a fair game where people guess the number of jelly beans in a jar.

- If you average the guesses of **100+ random people**, you'll likely be **closer to the truth** than any single person.
This is ensemble power in action:
- Everyone makes an **independent guess** (like a tree predicting y)
- You **aggregate** their answers (like averaging tree predictions)